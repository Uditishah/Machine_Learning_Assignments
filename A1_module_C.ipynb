{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z7LhP0LeX9T"
      },
      "source": [
        "## Module submission header\n",
        "### Submission preparation instructions\n",
        "_Completion of this header is mandatory, subject to a 2-point deduction to the assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Module submission group__. It is required to fill out descriptive notes pertaining to any tutoring support received in the completion of this submission under the __Additional submission comments__ section at the bottom of the header. If no tutoring support was received, leave NA in place. You may as well list other optional comments pertaining to the submission at bottom. _Any distruption of this header's formatting will make your group liable to the 2-point deduction._\n",
        "\n",
        "### Module submission group\n",
        "- Group member 1\n",
        "    - Name: Marissa Lynch\n",
        "    - Email: ml3758@drexel.edu\n",
        "- Group member 2\n",
        "    - Name: Uditi Shah\n",
        "    - Email: us54@drexel.edu\n",
        "- Group member 3\n",
        "    - Name: Yifan Wang\n",
        "    - Email: yw827@drexel.edu\n",
        "\n",
        "### Additional submission comments\n",
        "- Tutoring support received: NA\n",
        "- Other (other): NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVrs1m4deX9V"
      },
      "source": [
        "# Assignment group 1: Textual feature extraction and numerical comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMECVjkqeX9W"
      },
      "source": [
        "## Module C _(35 points)_ Similarity of word usage across a document\n",
        "\n",
        "Here we'll be building up some code to discover how different terms are utilized similarly across a document. For this, our first task will be to create a word frequency counting function.\n",
        "\n",
        "__C1.__ _(12 points)_ Define a function called `count_words(paragraph, pos = True, lemma = True)` that `return`s a `Counter()` called `frequency`. In `frequency`, each key will consist of a `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`. The `Counter()` should simply contain the number of times each `heading` is observed in the `paragraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DOjmDXjgeX9W"
      },
      "outputs": [],
      "source": [
        "# C1:Function(12/12)\n",
        "\n",
        "from collections import Counter\n",
        "import spacy, json, re\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def count_words(paragraph, pos = True, lemma = True):\n",
        "\n",
        "    #---Your code starts here\n",
        "      doc = nlp(paragraph)\n",
        "      frequency = Counter()\n",
        "\n",
        "      for word in doc:\n",
        "        text = word.lemma_ if lemma else word.text\n",
        "\n",
        "        tag = word.pos_ if pos else \"\"\n",
        "\n",
        "        heading = (text, tag)\n",
        "\n",
        "        frequency[heading] += 1\n",
        "    #---Your code ends here\n",
        "\n",
        "      return frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4B3XfR3eX9X"
      },
      "source": [
        "Let's make sure your function works by testing it on a short sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D73mjCzOeX9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f06539-371b-4feb-b890-94ac1a733011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({('the', 'DET'): 2,\n",
              "         ('quick', 'ADJ'): 1,\n",
              "         ('brown', 'ADJ'): 1,\n",
              "         ('fox', 'NOUN'): 1,\n",
              "         ('jump', 'VERB'): 1,\n",
              "         ('over', 'ADP'): 1,\n",
              "         ('lazy', 'ADJ'): 1,\n",
              "         ('dog', 'NOUN'): 1,\n",
              "         ('.', 'PUNCT'): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# C1:SanityCheck\n",
        "\n",
        "count_words(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVLz1QdteX9Y"
      },
      "source": [
        "__C2.__ _(8 pts)_ Next, define a function called `book_TDM(book_id, pos = True, lemma = True)` and copy into it the TDM-producing code from __Section 2.1.5.1__ of the lecture notes, now `return`-ing `TDM` and `all_words`. Once copied, modify this function to call `count_words` appropriately, now passing through the user of `book_TDM`'s specified `lemma` and `pos` arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Va7NqGemeX9Z"
      },
      "outputs": [],
      "source": [
        "# C2:Function(8/8)\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "def book_TDM(book_id, pos = True, lemma = True):\n",
        "\n",
        "    #---Your code starts here---\n",
        "    file_path = os.path.join('sample_data', f'{book_id}.txt')\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        book_content = file.read()\n",
        "\n",
        "    paragraphs = book_content.split('\\n\\n')\n",
        "\n",
        "    all_words = []\n",
        "\n",
        "    term_document_counts = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        frequency = count_words(paragraph, pos, lemma)\n",
        "        all_words.extend(frequency.keys())\n",
        "        term_document_counts.append(frequency)\n",
        "\n",
        "    all_words = list(set(all_words))\n",
        "\n",
        "    TDM = np.zeros((len(all_words), len(paragraphs)))\n",
        "\n",
        "    for j, frequency in enumerate(term_document_counts):\n",
        "        for i, word in enumerate(all_words):\n",
        "            TDM[i, j] = frequency[word]\n",
        "    #---Your code ends here---\n",
        "\n",
        "    return(TDM, all_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3_X0M0XeX9Z"
      },
      "source": [
        "\n",
        "To test your code's function, let's process `book_id = 84` with both of `pos = True` and `lemma = True` and print out the `TDM`'s `.shape` attribute and the first ten terms in `all_words`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UjSXY32AeX9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564170fe-39dd-4bcc-8fc9-ffdfe2cc46fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('retreat', 'NOUN'),\n",
              " ('triumphant', 'ADJ'),\n",
              " ('dizzy', 'ADJ'),\n",
              " ('hour', 'NOUN'),\n",
              " ('guilt', 'NOUN'),\n",
              " ('utterance', 'NOUN'),\n",
              " ('accident', 'NOUN'),\n",
              " ('venom', 'NOUN'),\n",
              " ('adopt', 'VERB'),\n",
              " ('distorted', 'ADJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# C2:SanityCheck\n",
        "\n",
        "TDM, terms = book_TDM(\"84\", pos = True, lemma = True)\n",
        "terms[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xghGDwpNeX9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c892ba-cf7d-46b1-a031-920c3af53a41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6262, 725)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# C2:SanityCheck\n",
        "\n",
        "TDM.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCwsVwiAeX9a"
      },
      "source": [
        "__C3.__ _(8 pts)_ Next, your job is to define two functions. The first is `sim(u,v)`, which shoud take two arbitrary numeric vectors and compute/output the `cosine_similarity`, as described in __Section 1.1.2.10__.  \n",
        "\n",
        "The second function is `term_sims(i, TDM)`, which should utilize the first function (`sim` function) to output a list of cosine similarity values (`sim_values`) between the word/row `i` and all others (rows) in the `TDM`.\n",
        "\n",
        "Note: each of these functions can be straightforwardly completed using a single line of code! Exhibit your knowledge of comprehensions and vectorization!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hmX1-PpKeX9b"
      },
      "outputs": [],
      "source": [
        "# C3:Function(4/8)\n",
        "import numpy as np\n",
        "def sim(u,v):\n",
        "\n",
        "    #---Your code starts here\n",
        "    cosine_similarity = np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "    #---Your code ends here\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WMoauCpzeX9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b08193-841b-42df-9434-3bfd1b8b0af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactly similar: 1.0\n",
            "Exactly dissimilar: -1.0\n",
            "In the middle: 0.0\n"
          ]
        }
      ],
      "source": [
        "# C3:SanityCheck\n",
        "\n",
        "print(\"Exactly similar:\", sim(np.array([1,2,3]), np.array([1,2,3])))\n",
        "print(\"Exactly dissimilar:\", sim(np.array([1,2,3]), np.array([-1,-2,-3])))\n",
        "print(\"In the middle:\", sim(np.array([1,1]), np.array([-1,1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "r5xwyGk3eX9b"
      },
      "outputs": [],
      "source": [
        "# C3:Function(4/8)\n",
        "\n",
        "def term_sims(i, TDM):\n",
        "\n",
        "    #---Your code starts here\n",
        "    sim_values = [sim(TDM[i], TDM[j]) for j in range(TDM.shape[0])]\n",
        "    #---Your code ends here\n",
        "\n",
        "    return sim_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FkO7YXELeX9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8284aa-bd4f-483d-b2a0-a1b21dfe4dc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07802743146408705,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.05407380704358752,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.034518691538566515,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.10362129429155138,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07647191129018724,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08606629658238706,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0629940788348712,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2357022603955158,\n",
              " 0.0,\n",
              " 0.07597371763975863,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.03456505649101418,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0563436169819011,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2357022603955158,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07106690545187015,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.09245003270420486,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2773500981126146,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13197938359912648,\n",
              " 0.049690399499995326,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0854357657716761,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0890870806374748,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.06913011298202835,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16222142113076254,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.11895607776997276,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07317617332646023,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07407407407407407,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0629940788348712,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07548513560963971,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.019849711139180454,\n",
              " 0.1259881576697424,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.04982900618346239,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.19245008972987526,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08606629658238703,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.06415002990995841,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07453559924999299,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.06804138174397717,\n",
              " 0.11605177063713189,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1178511301977579,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1259881576697424,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16169041669088866,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.03135736227945324,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08084520834544433,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1781741612749496,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07647191129018724,\n",
              " 0.19245008972987526,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08466675133346033,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.15475974959261785,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1421338109037403,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.03005565448891441,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2721655269759087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.09245003270420486,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2357022603955158,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1178511301977579,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14907119849998599,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.10050378152592121,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.041666666666666664,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.045679247321835446,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.19245008972987526,\n",
              " 0.013256666361597634,\n",
              " 0.048112522432468816,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.19245008972987526,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.037986858819879316,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0890870806374748,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.021652326748721033,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13900960937138318,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.19245008972987526,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14907119849998599,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.11785113019775792,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.052378280087892415,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0445435403187374,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08606629658238703,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0890870806374748,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.14907119849998599,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07544645503543211,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.10022524857043014,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.16666666666666666,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08606629658238703,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1301200097264711,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.13608276348795434,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.11605177063713189,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.07647191129018724,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.05555555555555555,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1126872339638022,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.11814686750138931,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.1111111111111111,\n",
              " 0.2222222222222222,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.06613964917771209,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.2357022603955158,\n",
              " 0.0,\n",
              " 0.13900960937138318,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.08606629658238706,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.047498033325273095,\n",
              " 0.3333333333333333,\n",
              " 0.0,\n",
              " 0.06375767130633382,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# C3:SanityCheck\n",
        "\n",
        "# Compare word/row 0 to all other (rows) in the TDM\n",
        "term_sims(0, TDM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoMhf6C8eX9c"
      },
      "source": [
        "__C4.__ _(7 pts)_ Finally, your goal now is to a write function, `most_similar(term, terms, TDM, top = 25)`, that utilizes `term_sims` to output a sorted list of the `top = N` terms (`top_n_terms`) most similar to one specified (`term`). The output data type should be a list of lists, with each inner list representing information for a similar term as: `[row_ix, similarity, term]`.\n",
        "\n",
        "\\[Hint: to locate the row containing the term of interest, utilize the list `.index()` method in application to the `terms` argument.\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1cHyltDseX9c"
      },
      "outputs": [],
      "source": [
        "# C4:Function(6/7)\n",
        "\n",
        "def most_similar(term, terms, TDM, top = 25):\n",
        "\n",
        "    #---Your code starts here---\n",
        "    try:\n",
        "        term_index = terms.index(term)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"The term '{term}' is not in the terms list.\")\n",
        "\n",
        "    term_vector = TDM[term_index]\n",
        "\n",
        "    similarities = []\n",
        "    for idx, other_term_vector in enumerate(TDM):\n",
        "        if idx != term_index:\n",
        "            similarity = sim(term_vector, other_term_vector)\n",
        "            similarities.append([idx, similarity, terms[idx]])\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_n_terms = similarities[:top]\n",
        "    #---Your code ends here---\n",
        "\n",
        "    return top_n_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNL8LRqeeX9c"
      },
      "source": [
        "Now, let's test your functions utility on a `TDM` produced for `book_id = 84` and exhibit the top 25 similar terms to both of `('monster', 'NOUN')` and `('beautiful', 'ADJ')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zPhCPuKgeX9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5fe2c4-4702-4b79-d06e-1a6b01b4038e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[143, 0.34299717028501764, ('posterity', 'NOUN')],\n",
              " [1816, 0.34299717028501764, ('formation', 'NOUN')],\n",
              " [2087, 0.34299717028501764, ('mutilate', 'VERB')],\n",
              " [2447, 0.34299717028501764, ('correct', 'VERB')],\n",
              " [4126, 0.34299717028501764, ('asseveration', 'NOUN')],\n",
              " [2522, 0.337510803485021, ('I', 'PRON')],\n",
              " [726, 0.3102526139970115, ('hideous', 'ADJ')],\n",
              " [4820, 0.2970442628930023, ('demoniacal', 'ADJ')],\n",
              " [6036, 0.2970442628930023, ('forgetfulness', 'NOUN')],\n",
              " [5266, 0.29194407123319405, (' ', 'SPACE')],\n",
              " [1979, 0.28020184134909754, ('that', 'SCONJ')],\n",
              " [810, 0.2794815423200416, ('!', 'PUNCT')],\n",
              " [3608, 0.27370899867041026, ('\\n', 'SPACE')],\n",
              " [4612, 0.2718318190198858, ('.', 'PUNCT')],\n",
              " [518, 0.27086782501514084, ('the', 'DET')],\n",
              " [4949, 0.27083418336873366, ('and', 'CCONJ')],\n",
              " [67, 0.2585438449975096, ('neck', 'NOUN')],\n",
              " [1500, 0.25724787771376323, ('narrative', 'NOUN')],\n",
              " [5530, 0.25724787771376323, ('loathsome', 'ADJ')],\n",
              " [4382, 0.25250143182442775, (',', 'PUNCT')],\n",
              " [2334, 0.2516271757683242, ('but', 'CCONJ')],\n",
              " [1169, 0.2507824668495261, (';', 'PUNCT')],\n",
              " [4666, 0.2500092955808841, ('from', 'ADP')],\n",
              " [5704, 0.24853214601036613, ('my', 'PRON')],\n",
              " [41, 0.24371527353101605, ('be', 'AUX')]]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# C4:SanityCheck\n",
        "\n",
        "most_similar(('monster', 'NOUN'), terms, TDM, top = 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QJLMWOYleX9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8c1a11-1fe0-46f2-c8e6-44670b463eed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2743, 0.40824829046386296, ('beneath', 'ADV')],\n",
              " [338, 0.37499999999999994, ('castle', 'NOUN')],\n",
              " [719, 0.37499999999999994, ('singularly', 'ADV')],\n",
              " [3908, 0.37499999999999994, ('horrid', 'ADJ')],\n",
              " [4784, 0.37499999999999994, ('Rotterdam', 'PROPN')],\n",
              " [393, 0.35355339059327373, ('fifth', 'ADJ')],\n",
              " [525, 0.35355339059327373, ('luxuriance', 'NOUN')],\n",
              " [571, 0.35355339059327373, ('dun', 'PROPN')],\n",
              " [676, 0.35355339059327373, ('Mannheim', 'PROPN')],\n",
              " [750, 0.35355339059327373, ('Mainz', 'PROPN')],\n",
              " [858, 0.35355339059327373, ('whence', 'ADP')],\n",
              " [1162, 0.35355339059327373, ('meandering', 'ADJ')],\n",
              " [1605, 0.35355339059327373, ('straight', 'ADJ')],\n",
              " [2582, 0.35355339059327373, ('populous', 'ADJ')],\n",
              " [3000, 0.35355339059327373, ('shrivel', 'VERB')],\n",
              " [3754, 0.35355339059327373, ('shipping', 'NOUN')],\n",
              " [3789, 0.35355339059327373, ('variegate', 'VERB')],\n",
              " [3808, 0.35355339059327373, ('delineate', 'ADJ')],\n",
              " [3848, 0.35355339059327373, ('pearly', 'ADJ')],\n",
              " [3913, 0.35355339059327373, ('willowy', 'PROPN')],\n",
              " [5193, 0.35355339059327373, ('whiteness', 'NOUN')],\n",
              " [6128, 0.35355339059327373, ('vineyard', 'NOUN')],\n",
              " [6142, 0.35355339059327373, ('steep', 'ADJ')],\n",
              " [4034, 0.33541019662496846, ('bank', 'NOUN')],\n",
              " [57, 0.33407655239053047, ('ruin', 'VERB')]]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# C4:SanityCheck\n",
        "\n",
        "most_similar(('beautiful', 'ADJ'), terms, TDM, top = 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CDhdawdjeX9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be058b9e-bc8c-4c5b-efcc-37573509c4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes\n"
          ]
        }
      ],
      "source": [
        "# C4:Inline\n",
        "\n",
        "# Comment on the ordered results returned in the sanity checks.\n",
        "# Do you think the algorithm is exhibiting sensible results? print \"Yes\" or \"No\"\n",
        "print(\"Yes\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}